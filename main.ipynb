{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHk18FMbZA95"
      },
      "source": [
        "# Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8IEx9utbwdJ"
      },
      "outputs": [],
      "source": [
        "# Run this cell if you are on external server like colab or kaggle. (Useful to train on GPU)\n",
        "# !git clone \"https://github.com/mathisemb/AdaIN.git\"\n",
        "# %cd AdaIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKT2Ig74ZA9_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtPuMab5ZA-A",
        "outputId": "d59c5414-fcd3-43ed-b979-a015afa94c01"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from utils.plot_tools import *\n",
        "from utils.dataloader_maker2 import dataloader_maker\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b-_NmViZA-B"
      },
      "source": [
        "## Training of the decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGx03QmPZA-C"
      },
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F1hI652ZA-C",
        "outputId": "dfdbb0b1-3c8e-4e82-cfc1-287a9e9d6325"
      },
      "outputs": [],
      "source": [
        "content_path = 'utils/datasets/MS_COCO_val'\n",
        "content_loader = dataloader_maker(folder_path= content_path, nb_of_images=32, batch_size=8)\n",
        "\n",
        "style_path = 'utils/datasets/wikiart/wikiart'\n",
        "style_loader = dataloader_maker(folder_path= style_path, nb_of_images=32, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "m93TmQs4ZA-D",
        "outputId": "befcef40-3976-49ae-979f-260f03c2227a"
      },
      "outputs": [],
      "source": [
        "# Show some images\n",
        "for img in content_loader :\n",
        "    plot_img(img[0][0],img[0][1], img[0][2])\n",
        "    break\n",
        "for img in style_loader :\n",
        "    plot_img(img[0][0],img[0][1], img[0][2])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsguKrCLZA-D"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftFdpNv2ZA-E",
        "outputId": "08569e42-005d-4ef6-cbb5-8df3694d6b0f"
      },
      "outputs": [],
      "source": [
        "from model import StyleTransfer\n",
        "lr = 3e-4\n",
        "lam = 2.\n",
        "model = StyleTransfer(lr=lr, lam=lam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5xaxtt2ZA-E"
      },
      "source": [
        "### Training the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfzClqGdZA-E",
        "outputId": "fdf1687c-98f8-4f60-a66a-da9fa8c688ee"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "nb_epochs = 1\n",
        "model.train_decoder(content_loader, style_loader, nb_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O85zI9-eZA-F"
      },
      "source": [
        "### Plot Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Szn1P6YXZA-F",
        "outputId": "f306442c-fb2f-4ef0-9542-104c520c9293"
      },
      "outputs": [],
      "source": [
        "plot_losses(model.content_LOSS, model.style_LOSS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvIyil4LZA-F"
      },
      "source": [
        "### Saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfiyTdUDZA-F"
      },
      "outputs": [],
      "source": [
        "# Will automatically be saved into the saving_path : 'model_checkpoints/Adain/'\n",
        "model.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgjnf41AZA-G"
      },
      "source": [
        "### Loading model to retrain it\n",
        "\n",
        "The loading is carried out in such a way that the era returns to where it was, and the list of all losses is continued and not overwritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "matwE-zaZA-G",
        "outputId": "cfdfe623-d621-4653-a3ea-9aab1d9902a5"
      },
      "outputs": [],
      "source": [
        "model = StyleTransfer(lr, lam)\n",
        "checkpoint_epoch = 50 #Choose which model you want to load, see them in the saving_path : 'model_checkpoints/Adain/'\n",
        "\n",
        "\n",
        "model.load(epoch=checkpoint_epoch)\n",
        "\n",
        "model.train_decoder(content_loader, style_loader, nb_epochs)\n",
        "model.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aWMcnO7ZA-G",
        "outputId": "1903d66a-176f-4f55-d95e-c6ad8bb84cff"
      },
      "outputs": [],
      "source": [
        "plt.plot(model.LOSS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQHyc8v3ZA-G"
      },
      "source": [
        "# Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP2n8OCwZA-H"
      },
      "source": [
        "## Load and preprocess the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g15XvFGKZA-H"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load the content and style images\n",
        "content_img = Image.open(\"images/content/000000000785.jpg\")\n",
        "style_img = Image.open(\"images/style/albert-marquet_life-class-at-the-cole-des-beaux-arts-fauvist-nude-1898.jpg\")\n",
        "\n",
        "# Preprocess the images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "content_tensor = preprocess(content_img).unsqueeze(0).to(device)\n",
        "style_tensor = preprocess(style_img).unsqueeze(0).to(device)\n",
        "\n",
        "#print(\"content_tensor:\", content_tensor.shape)\n",
        "#print(\"style_tensor:\", style_tensor.shape)\n",
        "\n",
        "#transforms.ToPILImage()(content_tensor.squeeze(0).cpu().clamp(0, 1)).show()\n",
        "#transforms.ToPILImage()(style_tensor.squeeze(0).cpu().clamp(0, 1)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTZEnFveZA-H"
      },
      "source": [
        "## Load and Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ61Mq8QZA-H",
        "outputId": "137def69-d4bb-48a4-dd73-d95dd02d8aed"
      },
      "outputs": [],
      "source": [
        "load_checkpoint = checkpoint_epoch + nb_epochs\n",
        "#load_checkpoint = 50\n",
        "print(load_checkpoint)\n",
        "\n",
        "model = StyleTransfer()\n",
        "model.load(load_checkpoint)\n",
        "\n",
        "with torch.no_grad():\n",
        "    stylized_img = model(content_tensor, style_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Z-bDisZA-I"
      },
      "source": [
        "## Print the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "oErHpTTNZA-I",
        "outputId": "aeeba60f-1299-495e-ea77-32d00f30ec35"
      },
      "outputs": [],
      "source": [
        "plot_img(content_tensor, style_tensor, stylized_img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
