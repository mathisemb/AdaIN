{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the cropped images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_img(img1, img2):\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Display the cropped image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img1.squeeze(0).permute(1, 2, 0))\n",
    "    plt.title('Cropped Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display the cropped style\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img2.squeeze(0).permute(1, 2, 0))\n",
    "    plt.title('Cropped Style')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images\n",
    "image_path = \"images/content/golden_gate.jpg\"\n",
    "#style_path = \"images/style/sketch.png\"\n",
    "style_path = \"images/style/Texturelabs_Grunge_337S.jpg\"\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "style = Image.open(style_path).convert('RGB')\n",
    "\n",
    "# Define the transformation to crop the images to size 224x224\n",
    "crop_transform = transforms.CenterCrop((224, 224))\n",
    "\n",
    "# Apply the transformation to the images\n",
    "cropped_image = crop_transform(image)\n",
    "cropped_style = crop_transform(style)\n",
    "\n",
    "# Convert PIL images to PyTorch tensors\n",
    "image_tensor = transforms.ToTensor()(cropped_image).unsqueeze(0)\n",
    "style_tensor = transforms.ToTensor()(cropped_style).unsqueeze(0)\n",
    "\n",
    "rawed_image = (image_tensor - image_tensor.mean())/image_tensor.std()\n",
    "image_with_style = rawed_image*style_tensor.std() + style_tensor.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(image_tensor, style_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(rawed_image, image_with_style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
